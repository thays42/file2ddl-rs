# file2ddl Project Overview

## Purpose
A high-performance CSV parser and DDL generator written in Rust that helps users prepare raw data files for loading into database tables.

## Current Status
**Phase 3 Complete** - Full type inference engine with DDL generation

## Key Commands

### Build & Test
```bash
# Build the project
cargo build

# Run tests
cargo test

# Run with verbose output for debugging
RUST_LOG=debug cargo run -- describe -i input.csv -v

# Run benchmarks
cargo bench

# Check code formatting
cargo fmt -- --check

# Run linter
cargo clippy
```

### Usage Examples
```bash
# Analyze CSV structure and types
cargo run -- describe -i data.csv

# Generate PostgreSQL DDL
cargo run -- describe -i data.csv --ddl

# Generate MySQL DDL with verbose output
cargo run -- describe -i data.csv --ddl --database mysql -v

# Parse CSV from stdin (Phase 1 command)
cat data.csv | cargo run -- parse

# Parse CSV with custom delimiter
cargo run -- parse -i tests/data/pipe_delimited.txt -o output.csv -d '|'

# Analyze pipe-delimited file
cargo run -- describe -i tests/data/pipe_delimited.txt -d '|' --ddl
```

## Project Structure
```
.                        # Project root
â”œâ”€â”€ Cargo.toml          # Package manifest
â”œâ”€â”€ Cargo.lock          # Dependency lock file
â”œâ”€â”€ README.md           # Project documentation
â”œâ”€â”€ CLAUDE.md           # AI assistant context
â”œâ”€â”€ design.md           # Design requirements
â”œâ”€â”€ plan.md             # Implementation plan
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs          # Entry point
â”‚   â”œâ”€â”€ lib.rs           # Library root
â”‚   â”œâ”€â”€ cli/             # CLI argument parsing (clap)
â”‚   â”œâ”€â”€ parser/          # CSV parsing logic
â”‚   â”‚   â”œâ”€â”€ mod.rs       # Main parser module
â”‚   â”‚   â””â”€â”€ streaming.rs # Streaming CSV implementation
â”‚   â”œâ”€â”€ analyzer/        # Type inference engine (Phase 3)
â”‚   â”‚   â”œâ”€â”€ mod.rs       # Main analyzer with describe command
â”‚   â”‚   â”œâ”€â”€ patterns.rs  # Type pattern matching
â”‚   â”‚   â”œâ”€â”€ column.rs    # Per-column analysis
â”‚   â”‚   â””â”€â”€ inference.rs # Streaming inference engine
â”‚   â”œâ”€â”€ ddl/            # DDL generation utilities
â”‚   â”œâ”€â”€ types/          # SQL type system and column statistics
â”‚   â”‚   â””â”€â”€ mod.rs       # SqlType enum and ColumnStats
â”‚   â””â”€â”€ utils/          # Utilities
â””â”€â”€ tests/
    â”œâ”€â”€ describe_integration_tests.rs # Integration tests for describe
    â”œâ”€â”€ integration/     # Parse command integration tests
    â””â”€â”€ data/           # Test data files
```

## Key Design Principles

### Performance & Memory
- **Streaming architecture**: Process files line-by-line, never load entire file into memory
- **Buffer size**: 8KB default for I/O operations
- **Max line length**: 1MB default (configurable with --max-line-length)
- **Iterator patterns**: Use Rust's lazy evaluation for efficiency

### Error Handling
- Graceful degradation with warnings
- Bad row collection with --badfile option
- Continue processing with --badmax option
- Non-zero exit on errors

## Development Phases

### âœ… Phase 1: Foundation (Complete)
- Project structure with Cargo dependencies
- CLI argument parsing using clap
- Basic streaming CSV reader
- Parse command with delimiter handling
- Test infrastructure

### âœ… Phase 2: Full Parse Command (Complete)
- Quote character handling (single/double)
- Quote escaping support
- NULL value transformation (--fnull, --tnull)
- Error handling with bad row collection
- Multiple encoding support
- RFC 4180 compliant output

### âœ… Phase 3: Type Inference Engine (Complete)
- Type detection for SQL types (BOOLEAN, SMALLINT, INTEGER, BIGINT, DOUBLE PRECISION, DATE, TIME, DATETIME, VARCHAR)
- Intelligent type promotion hierarchy
- Pattern-based recognition with regex
- Date/time format parsing with configurable formats
- Streaming analysis without loading entire files into memory
- Column statistics collection (null counts, sample values, min/max)
- Describe command with table output and DDL generation

### âœ… Phase 4: Describe Command & DDL (Complete)
- DDL generation for PostgreSQL, MySQL, Netezza
- Column statistics collection with null percentage
- Verbose logging for type promotions
- Configurable null value detection
- Column name sanitization for SQL compliance

### ðŸ“‹ Phase 5: Optimization (Future)
- Performance benchmarks with criterion
- Memory optimization profiling
- Large file processing optimization

## Test Data
- `tests/data/simple.csv` - Basic CSV file with id, name, age, active columns
- `tests/data/pipe_delimited.txt` - Pipe-delimited file
- `tests/data/type_inference.csv` - Complex types for testing inference
- `tests/data/promotions.csv` - Type promotion testing
- `tests/data/nulls.csv` - NULL value handling
- `tests/data/quoted.csv` - Quote handling testing

## Important Implementation Notes

### Type Inference Hierarchy
```
BOOLEAN -> SMALLINT -> INTEGER -> BIGINT -> DOUBLE PRECISION -> VARCHAR
DATE -> VARCHAR
TIME -> VARCHAR
DATETIME -> VARCHAR
```

### Supported SQL Types
- BOOLEAN (configurable with --ftrue/--ffalse)
- SMALLINT (-32,768 to 32,767)
- INTEGER (-2,147,483,648 to 2,147,483,647)
- BIGINT (-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807)
- DOUBLE PRECISION
- DATE (%Y-%m-%d default)
- TIME (%H:%M:%S default)
- DATETIME (%Y-%m-%d %H:%M:%S default)
- VARCHAR(n)

## Dependencies
- **clap**: CLI argument parsing with derive macros
- **csv**: RFC 4180 compliant CSV handling
- **serde**: Serialization framework
- **chrono**: Date/time parsing and formatting
- **regex**: Pattern matching for type inference
- **encoding_rs**: Character encoding support
- **anyhow/thiserror**: Comprehensive error handling
- **log/env_logger**: Structured logging framework

## Testing Strategy
- **Unit tests**: 24+ tests for individual functions and modules
- **Integration tests**: 9+ end-to-end command tests
- **Type inference tests**: Comprehensive coverage of all SQL types
- **Pattern matching tests**: Boolean, numeric, date/time validation
- **Error handling tests**: NULL values, type promotions, malformed data
- **Multi-database tests**: PostgreSQL, MySQL, Netezza DDL generation

## Current Capabilities
- âœ… **Parse Command**: Stream CSV with configurable delimiters, quotes, null handling
- âœ… **Describe Command**: Full type inference with statistical analysis
- âœ… **DDL Generation**: CREATE TABLE statements for major databases
- âœ… **Type System**: Complete SQL type hierarchy with intelligent promotions
- âœ… **Streaming Architecture**: Memory-efficient processing of large files
- âœ… **Error Resilience**: Graceful handling of malformed data with limits